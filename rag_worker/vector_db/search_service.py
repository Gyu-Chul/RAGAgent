"""
ê²€ìƒ‰ ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""

import logging
import time
from typing import List, Dict, Any, Optional
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import AnnSearchRequest, RRFRanker

from .config import EMBEDDING_MODELS
from .collection_manager import MilvusConnectionManager
from .embedding_service import BM25ModelCache, DenseEmbedder
from .exceptions import SearchError, ModelLoadError
from .types import SearchInput, SearchResult, SearchResultItem

logger = logging.getLogger(__name__)


class SparseQueryEmbedder:
    """í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„± í´ëž˜ìŠ¤"""

    def __init__(self, collection_name: str) -> None:
        """
        SparseQueryEmbedder ì´ˆê¸°í™”

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

        Raises:
            ModelLoadError: BM25 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        """
        self.collection_name: str = collection_name
        self.bm25 = BM25ModelCache.get(collection_name)

        if self.bm25 is None:
            raise ModelLoadError(
                f"BM25 model not found for collection '{collection_name}'. "
                f"Please run embedding first."
            )

    def embed_query(self, query: str) -> Dict[int, float]:
        """
        ì¿¼ë¦¬ë¥¼ í¬ì†Œ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬

        Returns:
            í¬ì†Œ ë²¡í„° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        """
        tokenized_query = query.split(" ")
        doc_scores = self.bm25.get_scores(tokenized_query)
        sparse_vec = {i: score for i, score in enumerate(doc_scores) if score > 0}
        return sparse_vec


class SearchService:
    """ê²€ìƒ‰ ì²˜ë¦¬ í†µí•© ì„œë¹„ìŠ¤ í´ëž˜ìŠ¤"""

    def __init__(self) -> None:
        """SearchService ì´ˆê¸°í™”"""
        self.client = MilvusConnectionManager.get_client()

    def _load_collection(self, collection_name: str) -> None:
        """
        ì»¬ë ‰ì…˜ ë¡œë“œ

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

        Raises:
            SearchError: ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            from pymilvus import Collection, connections

            # PyMilvus ì—°ê²° í™•ì¸
            MilvusConnectionManager.ensure_connection()

            # ì»¬ë ‰ì…˜ ë¡œë“œ
            collection = Collection(collection_name)
            collection.load()
            logger.info(f"âœ… Collection '{collection_name}' loaded successfully")

        except Exception as e:
            raise SearchError(f"Failed to load collection: {e}") from e

    def search(self, input_data: SearchInput) -> SearchResult:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë°€ì§‘ + í¬ì†Œ ë²¡í„°)

        Args:
            input_data: ê²€ìƒ‰ ìž…ë ¥

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        start_time: float = time.time()

        try:
            logger.info(
                f"â–¶ï¸ Starting hybrid search in collection: {input_data['collection_name']}"
            )

            # 0. ì»¬ë ‰ì…˜ ë¡œë“œ
            self._load_collection(input_data["collection_name"])

            # 1. ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„° ìƒì„±
            logger.info("Generating dense query vector...")
            dense_vector = self._generate_dense_vector(
                input_data["query"], input_data["model_key"]
            )

            # 2. í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„± (BM25 ëª¨ë¸ ìžë™ ìƒì„±)
            logger.info("Generating sparse query vector (BM25)...")
            sparse_vector = self._generate_sparse_vector(
                input_data["query"], input_data["collection_name"]
            )

            # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            logger.info("Executing hybrid search...")
            results = self._execute_hybrid_search(
                collection_name=input_data["collection_name"],
                dense_vector=dense_vector,
                sparse_vector=sparse_vector,
                top_k=input_data["top_k"],
                filter_expr=input_data.get("filter_expr"),
            )

            elapsed_time = time.time() - start_time
            logger.info(
                f"âœ… Search completed: {len(results)} results found in {elapsed_time:.2f}s"
            )

            return SearchResult(
                success=True,
                query=input_data["query"],
                collection_name=input_data["collection_name"],
                total_results=len(results),
                results=results,
                elapsed_time=elapsed_time,
                message=f"Found {len(results)} results",
                error=None,
            )

        except (ModelLoadError, SearchError, Exception) as e:
            elapsed_time = time.time() - start_time
            logger.error(f"âŒ Search failed: {e}")

            return SearchResult(
                success=False,
                query=input_data.get("query", ""),
                collection_name=input_data.get("collection_name", ""),
                total_results=0,
                results=[],
                elapsed_time=elapsed_time,
                message=None,
                error=str(e),
            )

    def _generate_dense_vector(self, query: str, model_key: str) -> List[float]:
        """
        ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„° ìƒì„±

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            model_key: ëª¨ë¸ í‚¤

        Returns:
            ë°€ì§‘ ë²¡í„°

        Raises:
            ModelLoadError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            model_config = EMBEDDING_MODELS.get(model_key)
            if not model_config:
                raise ModelLoadError(f"Model config not found for key: {model_key}")

            device = "cuda" if torch.cuda.is_available() else "cpu"

            embedder = HuggingFaceEmbeddings(
                model_name=model_config["model_name"],
                model_kwargs={"device": device, "trust_remote_code": True},
                encode_kwargs={"normalize_embeddings": True},
            )

            return embedder.embed_query(query)

        except Exception as e:
            raise ModelLoadError(f"Failed to generate dense vector: {e}") from e

    def _generate_sparse_vector(
        self, query: str, collection_name: str
    ) -> Dict[int, float]:
        """
        í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„±

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            í¬ì†Œ ë²¡í„°

        Raises:
            ModelLoadError: BM25 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        """
        try:
            sparse_embedder = SparseQueryEmbedder(collection_name)
            return sparse_embedder.embed_query(query)
        except ModelLoadError:
            # BM25 ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìžë™ìœ¼ë¡œ ìƒì„±
            logger.warning(f"âš ï¸ BM25 model not found for '{collection_name}'. Generating...")
            self._build_bm25_model(collection_name)

            # ìž¬ì‹œë„
            sparse_embedder = SparseQueryEmbedder(collection_name)
            return sparse_embedder.embed_query(query)

    def _build_bm25_model(self, collection_name: str) -> None:
        """
        ì»¬ë ‰ì…˜ì˜ ë°ì´í„°ë¡œë¶€í„° BM25 ëª¨ë¸ ìƒì„± ë° ìºì‹±

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

        Raises:
            SearchError: BM25 ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        from rank_bm25 import BM25Okapi

        try:
            logger.info(f"ðŸ”¨ Building BM25 model for collection: {collection_name}")

            # ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            from pymilvus import Collection

            collection = Collection(collection_name)

            # í…ìŠ¤íŠ¸ í•„ë“œ ì¿¼ë¦¬ (text í•„ë“œë§Œ í•„ìš”)
            query_result = collection.query(
                expr="pk >= 0",  # ëª¨ë“  ë°ì´í„°
                output_fields=["text"],
                limit=16384  # Milvus ìµœëŒ€ limit
            )

            if not query_result:
                raise SearchError(f"No documents found in collection '{collection_name}'")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í† í°í™”
            texts = [item["text"] for item in query_result if "text" in item]
            tokenized_corpus = [text.split(" ") for text in texts]

            logger.info(f"ðŸ“š Loaded {len(texts)} documents for BM25 model")

            # BM25 ëª¨ë¸ ìƒì„±
            bm25_model = BM25Okapi(tokenized_corpus)

            # ìºì‹œì— ì €ìž¥
            BM25ModelCache.set(collection_name, bm25_model)

            logger.info(f"âœ… BM25 model built and cached for '{collection_name}'")

        except Exception as e:
            raise SearchError(f"Failed to build BM25 model: {e}") from e

    def _execute_dense_search(
        self,
        collection_name: str,
        dense_vector: List[float],
        top_k: int,
        filter_expr: Optional[str] = None,
    ) -> List[SearchResultItem]:
        """
        ë°€ì§‘ ë²¡í„°ë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ (BM25 fallback)

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            dense_vector: ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„°
            top_k: ê²°ê³¼ ê°œìˆ˜
            filter_expr: í•„í„° í‘œí˜„ì‹ (ì„ íƒ)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Raises:
            SearchError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
        """
        try:
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            search_params: Dict[str, Any] = {
                "collection_name": collection_name,
                "data": [dense_vector],
                "anns_field": "dense",
                "limit": top_k,
                "output_fields": ["*"],
                "search_params": {"metric_type": "COSINE", "params": {"ef": 128}},
            }

            # í•„í„° ì¶”ê°€ (ìžˆì„ ê²½ìš°)
            if filter_expr:
                search_params["filter"] = filter_expr

            res = self.client.search(**search_params)

            if not res or not res[0]:
                return []

            # ê²°ê³¼ í¬ë§·íŒ…
            return self._format_results(res[0])

        except Exception as e:
            raise SearchError(f"Dense search execution failed: {e}") from e

    def _execute_hybrid_search(
        self,
        collection_name: str,
        dense_vector: List[float],
        sparse_vector: Dict[int, float],
        top_k: int,
        filter_expr: Optional[str] = None,
    ) -> List[SearchResultItem]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (RRF ëž­ì»¤ ì‚¬ìš©)

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            dense_vector: ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„°
            sparse_vector: í¬ì†Œ ì¿¼ë¦¬ ë²¡í„°
            top_k: ê²°ê³¼ ê°œìˆ˜
            filter_expr: í•„í„° í‘œí˜„ì‹ (ì„ íƒ)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Raises:
            SearchError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
        """
        try:
            # ë°€ì§‘ ë²¡í„° ê²€ìƒ‰ ìš”ì²­
            dense_req = AnnSearchRequest(
                data=[dense_vector],
                anns_field="dense",
                limit=top_k,
                param={"metric_type": "COSINE", "params": {"ef": 128}},
            )

            # í¬ì†Œ ë²¡í„° ê²€ìƒ‰ ìš”ì²­
            sparse_req = AnnSearchRequest(
                data=[sparse_vector],
                anns_field="sparse",
                limit=top_k,
                param={"metric_type": "IP"},
            )

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            search_params: Dict[str, Any] = {
                "collection_name": collection_name,
                "reqs": [dense_req, sparse_req],
                "ranker": RRFRanker(),
                "limit": top_k,
                "output_fields": ["*"],
            }

            # í•„í„° ì¶”ê°€ (ìžˆì„ ê²½ìš°)
            if filter_expr:
                search_params["filter"] = filter_expr

            res = self.client.hybrid_search(**search_params)

            if not res or not res[0]:
                return []

            # ê²°ê³¼ í¬ë§·íŒ…
            return self._format_results(res[0])

        except Exception as e:
            raise SearchError(f"Hybrid search execution failed: {e}") from e

    def _format_results(self, hits: List[Any]) -> List[SearchResultItem]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…

        Args:
            hits: ê²€ìƒ‰ ížˆíŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            í¬ë§·íŒ…ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results: List[SearchResultItem] = []

        for hit in hits:
            fields = hit.entity.fields.copy()

            # 'text' -> 'code' ë³€í™˜
            code = fields.pop("text", "")

            # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
            fields.pop("pk", None)
            fields.pop("dense", None)
            fields.pop("sparse", None)

            # ìŠ¤ì½”ì–´ ì¶”ê°€
            score = getattr(hit, "distance", None)

            result_item: SearchResultItem = SearchResultItem(
                code=code,
                file_path=fields.get("file_path", ""),
                name=fields.get("name", ""),
                start_line=fields.get("start_line", 0),
                end_line=fields.get("end_line", 0),
                type=fields.get("type", ""),
                _source_file=fields.get("_source_file", ""),
                score=score,
            )

            results.append(result_item)

        return results
