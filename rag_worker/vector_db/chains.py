import time, json, os, torch
from typing import List, Any, Literal

from langchain_core.documents import Document
from langchain_milvus import Milvus
from langchain_core.runnables import RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
from rank_bm25 import BM25Okapi
from pydantic import BaseModel, Field

import config
from db_utils import client # MilvusClient ì§ì ‘ ì‚¬ìš©ì„ ìœ„í•´ import

# --- 1. ì„ë² ë”© ì²´ì¸ (Embedding Chain) ---

class EmbeddingInput(BaseModel):
    """ì„ë² ë”© ì²´ì¸ì˜ ì…ë ¥ ëª¨ë¸"""
    json_path: str = Field(description="ë°ì´í„°ê°€ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œ")
    collection_name: str = Field(description="ì €ì¥í•  Milvus ì»¬ë ‰ì…˜ ì´ë¦„")
    model_key: str = Field(default=config.DEFAULT_MODEL_KEY, description="config.pyì— ì •ì˜ëœ ëª¨ë¸ í‚¤")

def _embedding_process(input_data: EmbeddingInput) -> dict:
    print(f"\nâ–¶ï¸ ì„ë² ë”© ì‹œì‘ : [Collection: {input_data.collection_name}]")
    start_time = time.time()

    # --- 1. ë°ì´í„° ë¡œë”© ë° ì¤€ë¹„ ---
    if not os.path.exists(input_data.json_path):
        return {"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {input_data.json_path}"}
    with open(input_data.json_path, "r", encoding="utf-8") as f:
        items = json.load(f)

    texts_to_embed = []
    metadata_list = []
    for item in items:
        page_content = item.get("code")
        if page_content and page_content.strip():   # code ì˜ì—­ì´ ë¹„ì–´ìˆì§€ ì•ŠëŠ”ì§€ í™•ì¸
            texts_to_embed.append(page_content)
            metadata_list.append({k: v for k, v in item.items() if k != "code"})

    if not texts_to_embed:
        return {"error": "JSON íŒŒì¼ì—ì„œ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}



    # --- 2. í¬ì†Œ ë²¡í„° ìƒì„± (rank_bm25 ì§ì ‘ ì‚¬ìš©) ---
    print("í¬ì†Œ ë²¡í„° ìƒì„± ì‹œì‘")

    tokenized_corpus = [doc.split(" ") for doc in texts_to_embed]  # ê°œë³„ ë‹¨ì–´ ê¸°ë°˜ì„ ìœ„í•´ ë¶„ë¦¬
    bm25 = BM25Okapi(tokenized_corpus)

    sparse_vectors = []
    for doc_tokens in tokenized_corpus:
        doc_scores = bm25.get_scores(doc_tokens)
        sparse_vec = {i: score for i, score in enumerate(doc_scores) if score > 0}
        sparse_vectors.append(sparse_vec)
    
    print("âœ… ëª¨ë“  í¬ì†Œ ë²¡í„° ìƒì„± ì™„ë£Œ")



    # --- 3. ë°€ì§‘ ë²¡í„° ìƒì„± (LangChain ì‚¬ìš©) ---
    print(f"{len(texts_to_embed)}ê°œ ë°€ì§‘ ë²¡í„° ìƒì„± ì‹œì‘")
    model_config = config.EMBEDDING_MODELS.get(input_data.model_key)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤ : {device}")

    dense_embedder = HuggingFaceEmbeddings(
        model_name=model_config["model_name"],
        model_kwargs={'device': device, 'trust_remote_code': True},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    ## HuggingFaceEmbeddings ëª¨ë¸ì´ GPU ìµœì í™” í¬í•¨í•´ì„œ ë²¡í„°í™” ì§„í–‰
    dense_vectors = dense_embedder.embed_documents(texts_to_embed)
    
    ### Vector ìƒì„±ì€ ì¢…ë£Œ
    ### ê·¸ê²ƒì„ Milvusì— ë„£ëŠ” ì‘ì—… - Memory Management & Stable Data Transferë¥¼ ìœ„í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ

    # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
    # VRAM ~8GB : 256     //  VRAM ~12GB : 512
    # VRAM ~16GB : 1024   //  VRAM 24GB~ : 2048 ~
    batch_size = 256
    inserted_count = 0

    for i in range(0, len(texts_to_embed), batch_size):
        batch_end = min(i + batch_size, len(texts_to_embed))
        print(f"  - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1} ~ {batch_end} / {len(texts_to_embed)}")

        # í˜„ì¬ ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì¡°ë¦½
        data_to_insert = []
        for j in range(i, batch_end):
            row = metadata_list[j].copy()
            row["text"] = texts_to_embed[j]
            row["dense"] = dense_vectors[j] 
            row["sparse"] = sparse_vectors[j]
            data_to_insert.append(row)

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì‚½ì…
        try:
            res = client.insert(
                collection_name=input_data.collection_name,
                data=data_to_insert
            )
            inserted_count += res['insert_count']
            print(f"  âœ… ë°°ì¹˜ ì‚½ì… ì„±ê³µ! (ì´ {inserted_count}ê°œ ì‚½ì…)")
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ë°°ì¹˜ {i} ì‚½ì… ì¤‘ ì˜¤ë¥˜: {e}"}

    elapsed = time.time() - start_time
    return {"success": True, "message": f"ğŸ‰ {len(texts_to_embed)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë° ì‚½ì… ì™„ë£Œ! (â±ï¸ {elapsed:.2f}ì´ˆ ì†Œìš”)"}

embedding_chain = RunnableLambda(_embedding_process)

# --- 2. ê²€ìƒ‰ ì²´ì¸ (Search Chain) ---

class SearchInput(BaseModel):
    query: str
    collection_name: str
    search_mode: Literal["hybrid", "vector", "bm25"] = "hybrid"
    model_key: str = Field(default=config.DEFAULT_MODEL_KEY)
    top_k: int = 3

def reciprocal_rank_fusion(results: List[List[Document]], k: int = 60) -> List[Document]:
    """Reciprocal Rank Fusion (RRF) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤."""
    fused_scores = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_content = doc.page_content # Document ê°ì²´ì˜ ë‚´ìš©ì„ í‚¤ë¡œ ì‚¬ìš©
            if doc_content not in fused_scores:
                fused_scores[doc_content] = {"score": 0, "doc": doc}
            fused_scores[doc_content]["score"] += 1 / (rank + k)

    reranked_results = sorted(fused_scores.values(), key=lambda x: x["score"], reverse=True)
    return [item["doc"] for item in reranked_results]


def _search_process(input_data: SearchInput) -> List[Document]:
    print(f"\nâ–¶ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ (ìˆ˜ë™ ë°©ì‹): [Mode: {input_data.search_mode}]")
    
    collection_name = input_data.collection_name
    query = input_data.query
    top_k = input_data.top_k
    
    # --- 1. ë°€ì§‘/í¬ì†Œ ë²¡í„° ìƒì„±ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ ---
    model_config = config.EMBEDDING_MODELS[input_data.model_key]
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    dense_embedder = HuggingFaceEmbeddings(
        model_name=model_config["model_name"],
        model_kwargs={'device': device, 'trust_remote_code': True},
        encode_kwargs={"normalize_embeddings": True}
    )

    # --- 2. ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ---
    
    # 2-1. ë°€ì§‘ ë²¡í„° ê²€ìƒ‰ (Dense Search)
    if input_data.search_mode in ["dense", "hybrid"]:
        print("  - ë°€ì§‘ ë²¡í„°ë¡œ ì˜ë¯¸ ê²€ìƒ‰ ì‹¤í–‰...")
        query_dense_vector = dense_embedder.embed_query(query)
        
        dense_search_params = {"metric_type": "COSINE", "params": {"ef": 128}}
        dense_results = client.search(
            collection_name=collection_name,
            data=[query_dense_vector],
            anns_field="dense", # ìŠ¤í‚¤ë§ˆ í•„ë“œëª…
            limit=top_k,
            search_params=dense_search_params,
            output_fields=["*"] # ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨
        )
    
    # 2-2. í¬ì†Œ ë²¡í„° ê²€ìƒ‰ (Sparse Search)
    if input_data.search_mode in ["sparse", "hybrid"]:
        print("  - í¬ì†Œ ë²¡í„°ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰...")
        # ì¿¼ë¦¬ë¥¼ í¬ì†Œ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”© ì‹œì™€ ë™ì¼í•œ ë¡œì§ í•„ìš”)
        # ì´ ë¶€ë¶„ì€ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì „ì²´ ì½”í¼ìŠ¤ë¡œ ë§Œë“  BM25 ëª¨ë¸ì„ ì¬ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
        temp_corpus = [query] # ì„ì‹œ ì½”í¼ìŠ¤
        tokenized_query = [doc.split(" ") for doc in temp_corpus]
        bm25 = BM25Okapi(tokenized_query)
        query_sparse_vector = {i: score for i, score in enumerate(bm25.get_scores(tokenized_query[0])) if score > 0}

        sparse_search_params = {"metric_type": "IP"}
        sparse_results = client.search(
            collection_name=collection_name,
            data=[query_sparse_vector],
            anns_field="sparse", # ìŠ¤í‚¤ë§ˆ í•„ë“œëª…
            limit=top_k,
            search_params=sparse_search_params,
            output_fields=["*"]
        )

    # --- 3. ê²€ìƒ‰ ê²°ê³¼ ì¡°í•© ë° ë°˜í™˜ ---
    final_docs = []
    
    if input_data.search_mode == "dense":
        print("âœ… ë°€ì§‘ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜")
        # Pymilvus ê²°ê³¼ë¥¼ LangChain Documentë¡œ ë³€í™˜
        hits = dense_results[0]
        final_docs = [Document(page_content=hit['entity'].get('text', ''), metadata={k:v for k,v in hit['entity'].items() if k != 'text'}) for hit in hits]

    elif input_data.search_mode == "sparse":
        print("âœ… í¬ì†Œ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜")
        hits = sparse_results[0]
        final_docs = [Document(page_content=hit['entity'].get('text', ''), metadata={k:v for k,v in hit['entity'].items() if k != 'text'}) for hit in hits]
    
    elif input_data.search_mode == "hybrid":
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì¡°í•© (RRF)...")
        dense_hits = [Document(page_content=hit['entity'].get('text', ''), metadata={k:v for k,v in hit['entity'].items() if k != 'text'}) for hit in dense_results[0]]
        sparse_hits = [Document(page_content=hit['entity'].get('text', ''), metadata={k:v for k,v in hit['entity'].items() if k != 'text'}) for hit in sparse_results[0]]
        
        # RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‘ ê²°ê³¼ ë­í‚¹ ì¡°í•©
        final_docs = reciprocal_rank_fusion([dense_hits, sparse_hits])
        final_docs = final_docs[:top_k] # ìµœì¢… top_k ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°

    return final_docs

def format_docs(docs: List[Document]) -> str:
    # ... (ê¸°ì¡´ format_docs í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥) ...
    # metadataë„ í•¨ê»˜ ë³´ì—¬ì£¼ë„ë¡ ê°œì„ í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
    if not docs:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_list = []
    # for i, doc in enumerate(docs):
    #     # file_path ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì¶œë ¥
    #     file_path = doc.metadata.get('file_path', 'N/A')
    #     formatted_list.append(
    #         f"  [{i+1}] ê²½ë¡œ: {file_path}\n      ë‚´ìš©: {doc.page_content[:200].replace(chr(10), ' ')}..."
    #     )
    return "\n".join(formatted_list)

search_chain = RunnableLambda(_search_process) | RunnableLambda(format_docs)




# def get_all_docs_from_collection(collection_name: str) -> List[str]:
#     """Milvus ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  'text' í•„ë“œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (BM25 ì¸ë±ì‹±ìš©)"""
#     if not client.has_collection(collection_name):
#         return []
#     # MilvusëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ëŠ” ì§ì ‘ì ì¸ ë°©ë²•ì´ ì œí•œì ì´ë¯€ë¡œ
#     # ì—¬ê¸°ì„œëŠ” IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°˜ë³µ ì¡°íšŒí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     # ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ìµœì í™”í•´ì•¼ í•©ë‹ˆë‹¤.
#     try:
#         # ì»¬ë ‰ì…˜ì˜ ì—”í‹°í‹° ìˆ˜ë¥¼ ë¨¼ì € í™•ì¸
#         count = client.get_collection_stats(collection_name).get("row_count", 0)
#         # ëª¨ë“  IDë¥¼ ì¡°íšŒí•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
#         results = client.query(
#             collection_name=collection_name,
#             filter="",
#             output_fields=["text"],
#             limit=int(count) if count > 0 else 10000 # countê°€ 0ì¼ ê²½ìš° ëŒ€ë¹„
#         )
#         return [res["text"] for res in results]
#     except Exception as e:
#         print(f"ì»¬ë ‰ì…˜ ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
#         return []

# def _search_process(input_data: SearchInput) -> List[Document]:
#     print(f"\nâ–¶ï¸ ë„¤ì´í‹°ë¸Œ ê²€ìƒ‰ ì‹œì‘: [Mode: {input_data.search_mode}]")
    
#     model_config = config.EMBEDDING_MODELS[input_data.model_key]
#     embeddings = HuggingFaceEmbeddings(
#         model_name=model_config["model_name"],
#         model_kwargs=model_config["kwargs"],
#         encode_kwargs={"normalize_embeddings": True}
#     )
    
#     vector_store = Milvus(
#         embedding_function=embeddings,
#         collection_name=input_data.collection_name,
#         connection_args={"uri": config.MILVUS_URI},
#         vector_field="dense_vector",
#         sparse_vector_field="sparse_vector",
#         text_field="text",
#         primary_field="pk"
#     )
    
#     # --- ğŸ‘‡ search_typeì„ ì´ìš©í•´ ë„¤ì´í‹°ë¸Œ ê²€ìƒ‰ ëª¨ë“œ ì œì–´ ---
#     search_type_map = {
#         "hybrid": "hybrid",
#         "vector": "similarity",
#         "bm25": "sparse"
#     }
#     stype = search_type_map.get(input_data.search_mode, "hybrid")
    
#     retriever = vector_store.as_retriever(
#         search_type=stype,
#         search_kwargs={"k": input_data.top_k}
#     )
    
#     return retriever.invoke(input_data.query)

# def format_docs(docs: List[Any]) -> str:
#     """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·íŒ…"""
#     if not docs:
#         return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
#     formatted_list = []
#     for i, doc in enumerate(docs):
#         formatted_list.append(
#             f"  [{i+1}] ë‚´ìš©: {doc.page_content[:200]}..."
#         )
#     return "\n".join(formatted_list)

# search_chain = RunnableLambda(_search_process)