import time, json, os, torch
from typing import List, Any, Dict

from langchain_core.documents import Document
from langchain_milvus import Milvus
from langchain_core.runnables import RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import AnnSearchRequest, RRFRanker
from rank_bm25 import BM25Okapi
from pydantic import BaseModel, Field

import config
from db_utils import client # MilvusClient ì§ì ‘ ì‚¬ìš©ì„ ìœ„í•´ import

# --- 1. ìž„ë² ë”© ì²´ì¸ (Embedding Chain) ---

class EmbeddingInput(BaseModel):
    """ìž„ë² ë”© ì²´ì¸ì˜ ìž…ë ¥ ëª¨ë¸"""
    json_path: str = Field(description="ë°ì´í„°ê°€ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œ")
    collection_name: str = Field(description="ì €ìž¥í•  Milvus ì»¬ë ‰ì…˜ ì´ë¦„")
    model_key: str = Field(default=config.DEFAULT_MODEL_KEY, description="config.pyì— ì •ì˜ëœ ëª¨ë¸ í‚¤")

# --- BM25 ëª¨ë¸ì„ ì €ìž¥í•˜ê³  ìž¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸ë©”ëª¨ë¦¬ ìºì‹œ ---
bm25_models = {}

def get_bm25_model(collection_name: str) -> BM25Okapi:
    """ì»¬ë ‰ì…˜ì˜ BM25 ëª¨ë¸ì„ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    if collection_name in bm25_models:
        return bm25_models[collection_name]

    print(f"  - ê²½ê³ : BM25 ëª¨ë¸ì´ ìºì‹œì— ì—†ìŠµë‹ˆë‹¤. '{collection_name}' ì»¬ë ‰ì…˜ ì „ì²´ë¥¼ ì½ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤ (ì‹œê°„ ì†Œìš”).")
    all_docs_from_db = client.query(
        collection_name=collection_name, filter="", output_fields=["text"], limit=16384
    )
    corpus = [res['text'] for res in all_docs_from_db]
    if not corpus:
        return None
        
    tokenized_corpus = [doc.split(" ") for doc in corpus]
    bm25 = BM25Okapi(tokenized_corpus)
    bm25_models[collection_name] = bm25 # ë‹¤ìŒ ì‚¬ìš©ì„ ìœ„í•´ ìºì‹œì— ì €ìž¥
    return bm25


def _embedding_process(input_data: EmbeddingInput) -> dict:
    print(f"\nâ–¶ï¸ ìž„ë² ë”© ì‹œìž‘ : [Collection: {input_data.collection_name}]")
    start_time = time.time()

    # --- 1. ë°ì´í„° ë¡œë”© ë° ì¤€ë¹„ ---
    if not os.path.exists(input_data.json_path):
        return {"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {input_data.json_path}"}
    with open(input_data.json_path, "r", encoding="utf-8") as f:
        items = json.load(f)

    texts_to_embed = []
    metadata_list = []
    for item in items:
        page_content = item.get("code")
        if page_content and page_content.strip():   # code ì˜ì—­ì´ ë¹„ì–´ìžˆì§€ ì•ŠëŠ”ì§€ í™•ì¸
            texts_to_embed.append(page_content)
            metadata_list.append({k: v for k, v in item.items() if k != "code"})

    if not texts_to_embed:
        return {"error": "JSON íŒŒì¼ì—ì„œ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}



    # --- 2. í¬ì†Œ ë²¡í„° ìƒì„± (rank_bm25 ì§ì ‘ ì‚¬ìš©) ---
    print("í¬ì†Œ ë²¡í„° ìƒì„± ì‹œìž‘")

    tokenized_corpus = [doc.split(" ") for doc in texts_to_embed]  # ê°œë³„ ë‹¨ì–´ ê¸°ë°˜ì„ ìœ„í•´ ë¶„ë¦¬
    bm25 = BM25Okapi(tokenized_corpus)

    bm25_models[input_data.collection_name] = bm25
    print("âœ… BM25 ëª¨ë¸ ìƒì„± ë° ìºì‹± ì™„ë£Œ.")

    sparse_vectors = []
    for doc_tokens in tokenized_corpus:
        doc_scores = bm25.get_scores(doc_tokens)
        sparse_vec = {i: score for i, score in enumerate(doc_scores) if score > 0}
        sparse_vectors.append(sparse_vec)
    
    print("âœ… ëª¨ë“  í¬ì†Œ ë²¡í„° ìƒì„± ì™„ë£Œ")



    # --- 3. ë°€ì§‘ ë²¡í„° ìƒì„± (LangChain ì‚¬ìš©) ---
    print(f"{len(texts_to_embed)}ê°œ ë°€ì§‘ ë²¡í„° ìƒì„± ì‹œìž‘")
    model_config = config.EMBEDDING_MODELS.get(input_data.model_key)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤ : {device}")

    dense_embedder = HuggingFaceEmbeddings(
        model_name=model_config["model_name"],
        model_kwargs={'device': device, 'trust_remote_code': True},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    ## HuggingFaceEmbeddings ëª¨ë¸ì´ GPU ìµœì í™” í¬í•¨í•´ì„œ ë²¡í„°í™” ì§„í–‰
    dense_vectors = dense_embedder.embed_documents(texts_to_embed)
    
    ### Vector ìƒì„±ì€ ì¢…ë£Œ
    ### ê·¸ê²ƒì„ Milvusì— ë„£ëŠ” ìž‘ì—… - Memory Management & Stable Data Transferë¥¼ ìœ„í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ

    # GPU ë©”ëª¨ë¦¬ì— ë§žì¶° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
    # VRAM ~8GB : 256     //  VRAM ~12GB : 512
    # VRAM ~16GB : 1024   //  VRAM 24GB~ : 2048 ~
    batch_size = 256
    inserted_count = 0

    for i in range(0, len(texts_to_embed), batch_size):
        batch_end = min(i + batch_size, len(texts_to_embed))
        print(f"  - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1} ~ {batch_end} / {len(texts_to_embed)}")

        # í˜„ìž¬ ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì¡°ë¦½
        data_to_insert = []
        for j in range(i, batch_end):
            row = metadata_list[j].copy()
            row["text"] = texts_to_embed[j]
            row["dense"] = dense_vectors[j] 
            row["sparse"] = sparse_vectors[j]
            data_to_insert.append(row)

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì‚½ìž…
        try:
            res = client.insert(
                collection_name=input_data.collection_name,
                data=data_to_insert
            )
            inserted_count += res['insert_count']
            print(f"  âœ… ë°°ì¹˜ ì‚½ìž… ì„±ê³µ! (ì´ {inserted_count}ê°œ ì‚½ìž…)")
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì‚½ìž… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ë°°ì¹˜ {i} ì‚½ìž… ì¤‘ ì˜¤ë¥˜: {e}"}

    elapsed = time.time() - start_time
    return {"success": True, "message": f"ðŸŽ‰ {len(texts_to_embed)}ê°œ ë¬¸ì„œ ìž„ë² ë”© ë° ì‚½ìž… ì™„ë£Œ! (â±ï¸ {elapsed:.2f}ì´ˆ ì†Œìš”)"}

embedding_chain = RunnableLambda(_embedding_process)

# --- 2. ê²€ìƒ‰ ì²´ì¸ (Search Chain) ---

class SearchInput(BaseModel):
    query: str
    collection_name: str
    ## search_mode: Literal["hybrid", "vector", "bm25"] = "hybrid"
    model_key: str = Field(default=config.DEFAULT_MODEL_KEY)
    top_k: int = 5

def _search_process(input_data: SearchInput) -> List[Dict[str, Any]]:
    """MilvusClientì˜ hybrid_searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print(f"\nâ–¶ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìž‘ (MilvusClient ë°©ì‹): [Collection: {input_data.collection_name}]")
    
    collection_name = input_data.collection_name
    query = input_data.query
    top_k = input_data.top_k
    
    # --- 1. ë°€ì§‘/í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„± ---
    # ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    print("  - ë°€ì§‘ ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì¤‘...")
    model_config = config.EMBEDDING_MODELS[input_data.model_key]
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dense_embedder = HuggingFaceEmbeddings(
        model_name=model_config["model_name"],
        model_kwargs={'device': device, 'trust_remote_code': True}
    )
    query_dense_vector = dense_embedder.embed_query(query)
    
    # --- í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„± ---
    print("  - BM25 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í¬ì†Œ ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì¤‘...")
    bm25 = get_bm25_model(collection_name)
    if bm25 is None:
        print("  - ì˜¤ë¥˜: í¬ì†Œ ë²¡í„° ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return []

    tokenized_query = query.split(" ")
    # ì¿¼ë¦¬ì˜ í† í°ë³„ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì—¬ í¬ì†Œ ë²¡í„° ìƒì„±
    doc_scores = bm25.get_scores(tokenized_query)
    query_sparse_vector = {i: score for i, score in enumerate(doc_scores) if score > 0}
    
    # --- 2. ê° ë²¡í„° í•„ë“œì— ëŒ€í•œ ê²€ìƒ‰ ìš”ì²­(AnnSearchRequest) ìƒì„± ---
    print("  - ë°€ì§‘/í¬ì†Œ ê²€ìƒ‰ ìš”ì²­ ìƒì„± ì¤‘...")

    # ë°€ì§‘ ë²¡í„° ê²€ìƒ‰ ìš”ì²­
    dense_req = AnnSearchRequest(
        data=[query_dense_vector],
        anns_field="dense",
        limit=top_k,
        param={"metric_type": "COSINE", "params": {"ef": 128}}
    )

    # í¬ì†Œ ë²¡í„° ê²€ìƒ‰ ìš”ì²­
    sparse_req = AnnSearchRequest(
        data=[query_sparse_vector], # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì§ì ‘ ì „ë‹¬
        anns_field="sparse",
        limit=top_k,
        param={"metric_type": "IP"}
    )

    # --- 3. RRF ëž­ì»¤ë¥¼ ì‚¬ìš©í•˜ì—¬ hybrid_search ì‹¤í–‰ ---
    print("  - client.hybrid_search ë¡œ RRF ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰...")
    res = client.hybrid_search(
        collection_name=collection_name,
        reqs=[dense_req, sparse_req],
        ranker=RRFRanker(),
        limit=top_k,
        output_fields=["*"]
    )

    if not res or not res[0]:
        return []

    print("  - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìµœì¢… JSON êµ¬ì¡°ë¡œ í¬ë§·íŒ… ì¤‘...")
    final_results = []
    for hit in res[0]:
        result_item = hit.entity.fields.copy()
        
        # 'text' í•„ë“œ -> 'code'
        result_item['code'] = result_item.pop('text', '')
        
        # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
        result_item.pop('pk', None)
        result_item.pop('dense', None)
        result_item.pop('sparse', None)
        
        final_results.append(result_item)
    
    return final_results



search_chain = RunnableLambda(_search_process)
