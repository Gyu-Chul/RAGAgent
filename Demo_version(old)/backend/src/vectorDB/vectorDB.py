from fastapi import APIRouter, UploadFile, File, Form, Query, HTTPException
from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType
from pymilvus.model.dense import SentenceTransformerEmbeddingFunction
from pydantic import BaseModel
from pathlib import Path
from typing import Any, Dict, List

import json, os, torch, tempfile, time

router = APIRouter()

###################### Connection Test
# MilvusClient ì¸ìŠ¤í„´ìŠ¤ ì „ì—­ ìƒì„± (ì¬ì‚¬ìš©)
client = MilvusClient(uri="http://127.0.0.1:19530", token="root:Milvus")

@router.get("/connection_test")
async def connection_test():
    try:
        # âœ… list_collections() í˜¸ì¶œë¡œ ì—°ê²° í™•ì¸
        collections = client.list_collections()

        return {
            "success": True,
            "message": "âœ… Milvus ì—°ê²° ì„±ê³µ! (MilvusClient ì‚¬ìš©)",
            "collections": collections
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨: {e}"
        }

###################### Count Entities
@router.get("/count_entities")
async def count_entities(collection_name: str = Query(..., description="Milvus ì»¬ë ‰ì…˜ ì´ë¦„")):
    try:
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not client.has_collection(collection_name):
            return {
                "success": False,
                "collection": collection_name,
                "message": f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }

        # ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ
        stats = client.get_collection_stats(collection_name)
        count = int(stats.get("row_count", 0))

        return {
            "success": True,
            "collection": collection_name,
            "entity_count": count,
            "message": f'ğŸ“Š Collection "{collection_name}" has {count} entities.'
        }

    except Exception as e:
        return {
            "success": False,
            "collection": collection_name,
            "error_type": "UnknownError",
            "message": f'âŒ ì—”í‹°í‹° ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}'
        }


###################### Create Collection
class CreateCollectionRequest(BaseModel):
    collection_name: str
    description: str = ""
    version: int = 1   # âœ… ê¸°ë³¸ê°’ ver1

@router.post("/create_collection")
async def create_collection_api(req: CreateCollectionRequest):
    try:
        if req.version == 0:
            result = create_collection_ver0(req.collection_name, req.description)
        elif req.version == 1:
            result = create_collection_ver1(req.collection_name, req.description)
        elif req.version == 2:
            result = create_collection_ver2(req.collection_name, req.description)
        elif req.version == 3:
            result = create_collection_ver3(req.collection_name, req.description)
        else:
            return {"success": False, "message": f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²„ì „: {req.version}"}

        return {"success": True, "message": result}

    except Exception as e:
        return {"success": False, "message": f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"}
    

def build_schema(fields, description=""):
    return {
        "fields": fields,
        "description": description
    }

# ê³µí†µ ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_index(collection_name: str, index_type: str, params: dict):
    client.create_index(
        collection_name=collection_name,
        field_name="embedding",
        index_params={
            "index_type": index_type,
            "metric_type": "L2",
            "params": params
        }
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ver0: IVF_FLAT + ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„° í•„ë“œ
def create_collection_ver0(collection_name: str, description: str = "") -> str:
    try:
        if client.has_collection(collection_name):
            return f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ì´ë¯¸ ì¡´ì¬í•¨"

        # âœ… í•„ë“œ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="type", dtype=DataType.VARCHAR, max_length=128),
            FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="start_line", dtype=DataType.INT64),
            FieldSchema(name="end_line", dtype=DataType.INT64),
            FieldSchema(name="code", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="file_path", dtype=DataType.VARCHAR, max_length=512),
        ]
        schema = CollectionSchema(fields=fields, description=description)

        # âœ… IndexParams ê°ì²´ ì‚¬ìš©
        index_params = client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="IVF_FLAT",
            metric_type="L2",
            params={"nlist": 128}
        )

        # âœ… MilvusClient í˜¸ì¶œ
        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=index_params
        )

        return f"âœ… ver0 ìƒì„± ì™„ë£Œ (IVF_FLAT ì¸ë±ìŠ¤ ì ìš©ë¨)"
    except Exception as e:
        return f"âŒ ver0 ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}"




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ver1: IVF_FLAT + ë‹¨ìˆœ text í•„ë“œ
def create_collection_ver1(collection_name: str, description: str = "") -> str:
    try:
        if client.has_collection(collection_name):
            return f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ì´ë¯¸ ì¡´ì¬í•¨"

        # âœ… í•„ë“œ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        ]
        schema = CollectionSchema(fields=fields, description=description)

        # âœ… IndexParams (IVF_FLAT)
        index_params = client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="IVF_FLAT",
            metric_type="L2",
            params={"nlist": 128}
        )

        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=index_params
        )
        return f"âœ… ver1 ìƒì„± ì™„ë£Œ (IVF_FLAT ì¸ë±ìŠ¤ ì ìš©ë¨)"
    except Exception as e:
        return f"âŒ ver1 ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ver2: HNSW + ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„° í•„ë“œ
def create_collection_ver2(collection_name: str, description: str = "") -> str:
    try:
        if client.has_collection(collection_name):
            return f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ì´ë¯¸ ì¡´ì¬í•¨"

        # âœ… í•„ë“œ ì •ì˜ (ver0ê³¼ ë™ì¼)
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="type", dtype=DataType.VARCHAR, max_length=128),
            FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="start_line", dtype=DataType.INT64),
            FieldSchema(name="end_line", dtype=DataType.INT64),
            FieldSchema(name="code", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="file_path", dtype=DataType.VARCHAR, max_length=512),
        ]
        schema = CollectionSchema(fields=fields, description=description)

        # âœ… IndexParams (HNSW)
        index_params = client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="HNSW",
            metric_type="L2",
            params={"M": 16, "efConstruction": 200}
        )

        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=index_params
        )
        return f"âœ… ver2 ìƒì„± ì™„ë£Œ (HNSW ì¸ë±ìŠ¤ ì ìš©ë¨)"
    except Exception as e:
        return f"âŒ ver2 ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ver3: HNSW + ë‹¨ìˆœ text í•„ë“œ
def create_collection_ver3(collection_name: str, description: str = "") -> str:
    try:
        if client.has_collection(collection_name):
            return f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ì´ë¯¸ ì¡´ì¬í•¨"

        # âœ… í•„ë“œ ì •ì˜ (ver1ê³¼ ë™ì¼)
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        ]
        schema = CollectionSchema(fields=fields, description=description)

        # âœ… IndexParams (HNSW)
        index_params = client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="HNSW",
            metric_type="L2",
            params={"M": 16, "efConstruction": 200}
        )

        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=index_params
        )
        return f"âœ… ver3 ìƒì„± ì™„ë£Œ (HNSW ì¸ë±ìŠ¤ ì ìš©ë¨)"
    except Exception as e:
        return f"âŒ ver3 ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}"



###################### Delete Collection
class DeleteCollectionRequest(BaseModel):
    collection_name: str

@router.delete("/delete_collection")
async def delete_collection(req: DeleteCollectionRequest):
    try:
        # âœ… ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not client.has_collection(req.collection_name):
            return {
                "success": False,
                "message": f"âŒ ì»¬ë ‰ì…˜ '{req.collection_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            }

        # âœ… MilvusClient drop
        client.drop_collection(req.collection_name)
        return {
            "success": True,
            "message": f"ğŸ—‘ï¸ ì»¬ë ‰ì…˜ '{req.collection_name}' ì‚­ì œ ì™„ë£Œ"
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        }






###################### List Collections    
@router.get("/list_collections")
async def list_collections():
    try:
        # âœ… MilvusClientë¡œ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ
        collections = client.list_collections()

        # âœ… ì»¬ë ‰ì…˜ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
        if not collections:
            return {
                "success": True,
                "collections": [],
                "count": 0,
                "message": "âš ï¸ í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
            }

        # âœ… ê²°ê³¼ ë°˜í™˜ (ì´ë¦„ ë¦¬ìŠ¤íŠ¸ í¬í•¨)
        return {
            "success": True,
            "collections": collections,  # âœ… ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì „ë‹¬
            "count": len(collections),
            "message": f"âœ… ì´ {len(collections)}ê°œì˜ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {', '.join(collections)}"
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ ì»¬ë ‰ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        }
    


###################### View Entitiy
@router.get("/view_entities")
async def view_entities(
    collection_name: str = Query(..., description="Milvus ì»¬ë ‰ì…˜ ì´ë¦„")
):
    try:
        # 1. ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not client.has_collection(collection_name):
            return {
                "success": False,
                "message": f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }

        # 2. ìŠ¤í‚¤ë§ˆ í•„ë“œ ì¡°íšŒ
        schema_info = client.describe_collection(collection_name)
        field_names = [f["name"] for f in schema_info["fields"]]

        # 3. ì¡°íšŒí•  í•„ë“œ ë™ì  êµ¬ì„±
        output_fields = ["id", "embedding"]
        for f in ["text", "code", "type", "name", "file_path", "start_line", "end_line"]:
            if f in field_names:
                output_fields.append(f)

        # 4. MilvusClient Query ì‹¤í–‰
        results = client.query(
            collection_name=collection_name,
            filter="", 
            output_fields=output_fields,
            limit=100
        )

        if not results:
            return {
                "success": True,
                "entities": [],
                "count": 0,
                "message": f"âš ï¸ '{collection_name}' ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # 5. ê²°ê³¼ ê°€ê³µ (numpy â†’ float ë³€í™˜ ë° ì¡´ì¬ í•„ë“œë§Œ ì¶”ê°€)
        entities = []
        for item in results:
            embedding = item.get("embedding", [])
            embedding_preview = [float(x) for x in embedding[:5]] if embedding else []

            entity = {
                "id": item.get("id"),
                "embedding_dim": len(embedding),
                "embedding_preview": embedding_preview
            }

            # âœ… ì¡´ì¬í•˜ëŠ” í•„ë“œë§Œ ì•ˆì „í•˜ê²Œ ì¶”ê°€
            for f in ["text", "code", "type", "name", "file_path", "start_line", "end_line"]:
                if f in field_names:
                    value = item.get(f)
                    if value is not None:
                        entity[f] = str(value)[:150] if isinstance(value, str) else value

            entities.append(entity)

        # 6. ì‘ë‹µ ë°˜í™˜
        return {
            "success": True,
            "message": f"âœ… '{collection_name}'ì—ì„œ {len(entities)}ê°œ ì—”í‹°í‹° ì¡°íšŒë¨.",
            "entities": entities,
            "count": len(entities)
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        }


###################### Delete Entity
class DeleteEntityRequest(BaseModel):
    collection_name: str
    entity_id: str

@router.delete("/delete_entity")
async def delete_entity(req: DeleteEntityRequest):
    try:
        # âœ… ì»¬ë ‰ì…˜ í™•ì¸
        if not client.has_collection(req.collection_name):
            return {
                "success": False,
                "message": f"âŒ ì»¬ë ‰ì…˜ '{req.collection_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            }

        # âœ… IDê°€ ìˆ«ì í˜•íƒœì¸ì§€ ê²€ì¦
        if not req.entity_id.isdigit():
            return {
                "success": False,
                "message": "âŒ ì‚­ì œ ì‹¤íŒ¨: ìˆ«ì IDë§Œ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }

        # âœ… í•„í„° ì¡°ê±´ ìƒì„±
        filter_expr = f"id in [{req.entity_id}]"

        # âœ… MilvusClient ì‚­ì œ í˜¸ì¶œ
        client.delete(
            collection_name=req.collection_name,
            filter=filter_expr
        )

        return {
            "success": True,
            "message": f"âœ… ì—”í‹°í‹° ID {req.entity_id} ì‚­ì œ ì™„ë£Œ"
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        }



#########################################################################################

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •ê°’ (í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆ)
DEFAULT_EMBED_BATCH = 256                     # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
DEFAULT_MAX_PAYLOAD_BYTES = 50 * 1024 * 1024  # gRPC 64MBë³´ë‹¤ ì—¬ìœ  ìˆê²Œ 50MB ëª©í‘œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒë‹¨ ì„¤ì •
HARD_CAP_CHARS = 65000  # ìŠ¤í‚¤ë§ˆ ëª» ì½ì„ ë•Œ ìµœí›„ ì•ˆì „ê°’(VarChar 65535 í•˜í•œì„  ê·¼ì²˜)

def _dtype_is_varchar(dt) -> bool:
    # dtê°€ enum, int, str ì–´ëŠ í˜•íƒœë“  'varchar' íŒë³„
    s = str(dt).lower()
    return ("varchar" in s) or (s.strip() in {"23", "varchar", "data_type.varchar"})

def _get_maxlen_from_field(f: dict) -> int | None:
    # params or type_params ì•ˆì—ì„œ max_length ì¶”ì¶œ (strì¼ ìˆ˜ ìˆìŒ)
    for key in ("params", "type_params"):
        params = f.get(key) or {}
        if "max_length" in params:
            try:
                return int(params["max_length"])
            except Exception:
                pass
    return None

def get_varchar_limits(schema_info: dict) -> dict[str, int]:
    """í•„ë“œë³„ VarChar max_length ë§¤í•‘. ëª» ì°¾ìœ¼ë©´ None (ë‚˜ì¤‘ì— í•˜ë“œìº¡ ì ìš©)."""
    limits = {}
    for f in schema_info.get("fields", []):
        if _dtype_is_varchar(f.get("data_type")):
            ml = _get_maxlen_from_field(f)
            if isinstance(ml, int) and ml > 0:
                limits[f["name"]] = ml
    return limits



def truncate_utf8(s: str, max_bytes: int) -> str:
    """UTF-8 ë°”ì´íŠ¸ ê¸°ì¤€ ì•ˆì „ ì ˆë‹¨(ë©€í‹°ë°”ì´íŠ¸ ê¹¨ì§ ë°©ì§€)."""
    if s is None or max_bytes is None:
        return s
    b = s.encode("utf-8")
    if len(b) <= max_bytes:
        return s
    ell = "â€¦[TRUNCATED]"
    ell_b = ell.encode("utf-8")
    keep = max_bytes - len(ell_b)
    if keep <= 0:
        return b[:max_bytes].decode("utf-8", errors="ignore")
    return b[:keep].decode("utf-8", errors="ignore") + ell


def truncate_chars(s: str, max_chars: int | None) -> str:
    if s is None or max_chars is None:
        return s
    if len(s) <= max_chars:
        return s
    ell = "â€¦[TRUNCATED]"
    keep = max(0, max_chars - len(ell))
    return (s[:keep] + ell) if keep > 0 else s[:max_chars]



def approx_row_bytes(vec, item: dict, include_text: bool) -> int:
    """gRPC í˜ì´ë¡œë“œ ê·¼ì‚¬ì¹˜: ë²¡í„°(float32) + í…ìŠ¤íŠ¸(ì˜µì…˜) + ì˜¤ë²„í—¤ë“œ."""
    vec_bytes = (len(vec) * 4) if hasattr(vec, "__len__") else 0  # float32 ê°€ì •
    text_bytes = 0
    if include_text:
        try:
            text_bytes = len(json.dumps(item, ensure_ascii=False).encode("utf-8"))
        except Exception:
            text_bytes = 0
    return vec_bytes + text_bytes + 512  # í—¤ë”/í•„ë“œ ì˜¤ë²„í—¤ë“œ ì—¬ìœ ì¹˜


def build_row_for_schema(item: dict, vec, field_names: set[str], varchar_limits: dict[str, int]):
    row = {"embedding": vec}

    if "text" in field_names:
        raw = json.dumps(item, ensure_ascii=False)
        ml = varchar_limits.get("text", HARD_CAP_CHARS)
        row["text"] = truncate_chars(raw, ml)

    if "code" in field_names:
        raw = item.get("code", "[NO CODE]")
        ml = varchar_limits.get("code", HARD_CAP_CHARS)
        row["code"] = truncate_chars(raw, ml)

    if "type" in field_names:
        raw = item.get("type", "unknown")
        ml = varchar_limits.get("type")  # ì¼ë°˜ì ìœ¼ë¡œ ì‘ìŒ, ì—†ìœ¼ë©´ ì œí•œ ì—†ìŒ
        row["type"] = truncate_chars(raw, ml) if ml else raw

    if "name" in field_names:
        raw = item.get("name", "unknown")
        ml = varchar_limits.get("name")
        row["name"] = truncate_chars(raw, ml) if ml else raw

    if "file_path" in field_names:
        raw = item.get("file_path", "unknown")
        ml = varchar_limits.get("file_path")
        row["file_path"] = truncate_chars(raw, ml) if ml else raw

    if "start_line" in field_names:
        row["start_line"] = int(item.get("start_line", 0))
    if "end_line" in field_names:
        row["end_line"] = int(item.get("end_line", 0))

    # (ì„ íƒ) ì˜ë¦¼ ì—¬ë¶€ í‘œì‹œ
    if "is_truncated" in field_names:
        def over(name: str) -> bool:
            if name not in row:
                return False
            lm = varchar_limits.get(name, HARD_CAP_CHARS if name in ("text", "code") else None)
            return lm is not None and len(row[name]) >= lm
        row["is_truncated"] = bool(over("text") or over("code"))

    return row


#################################################### ì—…ë¡œë“œë¥¼ í†µí•œ json ì½ê¸° ë°©ë²•
# @router.post("/embed_json_file")
# async def embed_json_file(
#     file: UploadFile = File(...),
#     collection_name: str = Form(...),
#     embed_batch_size: int = Form(DEFAULT_EMBED_BATCH),              # ì„ë² ë”© ë°°ì¹˜
#     max_payload_bytes: int = Form(DEFAULT_MAX_PAYLOAD_BYTES),       # gRPC í˜ì´ë¡œë“œ ì»·
# ):
#     """
#     ëŒ€ê·œëª¨ JSON:
#     - ì„ë² ë”©: embed_batch_sizeë¡œ ìª¼ê°œì„œ ì²˜ë¦¬
#     - ì‚½ì…: gRPC í˜ì´ë¡œë“œë¥¼ max_payload_bytes ì´í•˜ê°€ ë˜ë„ë¡ ë°”ì´íŠ¸ ê¸°ì¤€ ë¶„í• 
#     - VarChar: ìŠ¤í‚¤ë§ˆ max_lengthì— ë§ì¶° UTF-8 ì•ˆì „ ì ˆë‹¨
#     - ì‹œê°„: ì„ë² ë”©/ì‚½ì…/ì „ì²´ ê²½ê³¼ í¬í•¨í•´ messageë¡œ ë°˜í™˜
#     """
#     try:
#         total_start = time.perf_counter()

#         # 1) íŒŒì¼ ê²€ì¦ + ì„ì‹œ ì €ì¥
#         if not file.filename.endswith(".json"):
#             return {"success": False, "message": "âŒ JSON íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."}

#         with tempfile.NamedTemporaryFile(delete=False, suffix=".json") as tmp:
#             tmp.write(await file.read())
#             tmp_path = tmp.name

#         # 2) JSON ë¡œë“œ
#         with open(tmp_path, "r", encoding="utf-8") as f:
#             data = json.load(f)
#         all_items = data if isinstance(data, list) else [data]
#         total_items = len(all_items)
#         if total_items == 0:
#             return {"success": False, "message": "âš ï¸ ë¹„ì–´ìˆëŠ” JSONì…ë‹ˆë‹¤."}

#         # 3) í™˜ê²½ ì¤€ë¹„
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         embedding_fn = SentenceTransformerEmbeddingFunction(
#             model_name="sentence-transformers/all-mpnet-base-v2",
#             device=device
#         )

#         if not client.has_collection(collection_name):
#             return {"success": False, "message": f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}

#         # ìŠ¤í‚¤ë§ˆ 1íšŒ ì¡°íšŒ
#         schema_info = client.describe_collection(collection_name)
#         field_names = {f["name"] for f in schema_info.get("fields", [])}
#         varchar_limits = get_varchar_limits(schema_info)

#         embed_elapsed_total = 0.0
#         insert_elapsed_total = 0.0
#         inserted_count = 0

#         # 4) ë°°ì¹˜ ì„ë² ë”© ë£¨í”„
#         for start in range(0, total_items, embed_batch_size):
#             end = min(start + embed_batch_size, total_items)
#             batch_items = all_items[start:end]
#             docs_as_strings = [json.dumps(item, ensure_ascii=False) for item in batch_items]

#             # ì„ë² ë”© ì‹œê°„ ì¸¡ì • (CUDA ë™ê¸°í™”ë¡œ ì •í™•ë„â†‘)
#             if device == "cuda":
#                 torch.cuda.synchronize()
#             t0 = time.perf_counter()
#             vectors = embedding_fn.encode_documents(docs_as_strings)
#             if device == "cuda":
#                 torch.cuda.synchronize()
#             embed_elapsed_total += (time.perf_counter() - t0)

#             if len(vectors) != len(batch_items):
#                 return {"success": False, "message": "âŒ ë²¡í„°/ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜(ë°°ì¹˜)"}

#             # 5) ì‚½ì…: ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  ì „ì†¡
#             buffer_rows = []
#             buffer_bytes = 0

#             include_text = ("text" in field_names)

#             for i, item in enumerate(batch_items):
#                 row = build_row_for_schema(item, vectors[i], field_names, varchar_limits)
#                 row_bytes = approx_row_bytes(vectors[i], item, include_text=include_text)

#                 # í˜„ì¬ ë²„í¼ + ìƒˆ í–‰ì´ í•œë„ë¥¼ ë„˜ìœ¼ë©´ ë¨¼ì € flush
#                 if buffer_rows and (buffer_bytes + row_bytes) > max_payload_bytes:
#                     t1 = time.perf_counter()
#                     client.insert(collection_name=collection_name, data=buffer_rows)
#                     insert_elapsed_total += (time.perf_counter() - t1)
#                     inserted_count += len(buffer_rows)
#                     buffer_rows = []
#                     buffer_bytes = 0

#                 buffer_rows.append(row)
#                 buffer_bytes += row_bytes

#             # ì”ì—¬ flush
#             if buffer_rows:
#                 t1 = time.perf_counter()
#                 client.insert(collection_name=collection_name, data=buffer_rows)
#                 insert_elapsed_total += (time.perf_counter() - t1)
#                 inserted_count += len(buffer_rows)

#         # 6) ì´ ì—”í‹°í‹° ìˆ˜
#         stats = client.get_collection_stats(collection_name)
#         total_count = int(stats["row_count"])

#         total_elapsed = time.perf_counter() - total_start

#         # search_basic_api ìŠ¤íƒ€ì¼ì˜ message
#         return {
#             "success": True,
#             "message": (
#                 f"ğŸ‰ {inserted_count}ê°œ ì—”í‹°í‹° ì‚½ì… ì™„ë£Œ "
#                 f"(â±ï¸ ì„ë² ë”© {embed_elapsed_total:.2f}s, ì‚½ì… {insert_elapsed_total:.2f}s, ì „ì²´ {total_elapsed:.2f}s; "
#                 f"ë°°ì¹˜ embed={embed_batch_size}, payloadâ‰¤{max_payload_bytes // (1024*1024)}MB)"
#             ),
#             "total_entities": total_count
#         }

#     except Exception as e:
#         return {"success": False, "message": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"}

#     finally:
#         if 'tmp_path' in locals() and os.path.exists(tmp_path):
#             os.remove(tmp_path)

##########################################################################################################################################################
############################################################################# ì‹¤ì œ ì„ë² ë”© api
##########################################################################################################################################################
class EmbedJsonRequest(BaseModel):
    json_path: str
    collection_name: str
    embed_batch_size: int = DEFAULT_EMBED_BATCH
    max_payload_bytes: int = DEFAULT_MAX_PAYLOAD_BYTES

@router.post("/embed_json_file")
async def embed_json_file(req: EmbedJsonRequest):
    """
    Content-Type: application/json ìš”ì²­ ì „ìš©
    ê¸°ì¡´ ë¡œì§ ìœ ì§€, íŒŒì¼ ì—…ë¡œë“œ ëŒ€ì‹  ì„œë²„ì— ì¡´ì¬í•˜ëŠ” JSON íŒŒì¼ ê²½ë¡œë¥¼ ë°›ìŒ
    """
    try:
        total_start = time.perf_counter()

        # 1) íŒŒì¼ ê²½ë¡œ ê²€ì¦
        if not req.json_path.endswith(".json"):
            return {"success": False, "message": "âŒ JSON íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤."}
        if not os.path.exists(req.json_path):
            return {"success": False, "message": f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {req.json_path}"}

        # 2) JSON ë¡œë“œ
        with open(req.json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        all_items = data if isinstance(data, list) else [data]
        total_items = len(all_items)
        if total_items == 0:
            return {"success": False, "message": "âš ï¸ ë¹„ì–´ìˆëŠ” JSONì…ë‹ˆë‹¤."}

        # 3) í™˜ê²½ ì¤€ë¹„
        device = "cuda" if torch.cuda.is_available() else "cpu"
        embedding_fn = SentenceTransformerEmbeddingFunction(
            model_name="sentence-transformers/all-mpnet-base-v2",
            device=device
        )

        if not client.has_collection(req.collection_name):
            return {"success": False, "message": f"âŒ ì»¬ë ‰ì…˜ '{req.collection_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}

        # ìŠ¤í‚¤ë§ˆ 1íšŒ ì¡°íšŒ
        schema_info = client.describe_collection(req.collection_name)       ### í•´ë‹¹í•˜ëŠ” DBì˜ ì •ë³´ ì¶”ì¶œ - ì–´ë–¤ fieldê°€ ìˆëŠ”ì§€ ì¦‰, ì–´ë–¤ ë©”íƒ€ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ”ì§€ ì •ë³´ í¬í•¨
        field_names = {f["name"] for f in schema_info.get("fields", [])}    ### ê·¸ ì •ë³´ì—ì„œ field nameì„ ê°€ì ¸ì™€ì„œ ì„ë² ë”©í•  ë•Œ ì–´ë–¤ êµ¬ì¡°ë¡œ ì„ë² ë”©í•  ê²ƒì¸ì§€ ìë™ ê²°ì • (ë¶„ê¸° ì—†ì´ ë²„ì „ ì²˜ë¦¬ ì„±ê³µ)
        varchar_limits = get_varchar_limits(schema_info)

        embed_elapsed_total = 0.0
        insert_elapsed_total = 0.0
        inserted_count = 0

        # 4) ë°°ì¹˜ ì„ë² ë”© ë£¨í”„
        for start in range(0, total_items, req.embed_batch_size):
            print("ì„ë² ë”© ì‘ì—…ì¤‘ ì…ë‹ˆë‹¤.")
            print(start,total_items)
            end = min(start + req.embed_batch_size, total_items)
            batch_items = all_items[start:end]
            docs_as_strings = [json.dumps(item, ensure_ascii=False) for item in batch_items]

            if device == "cuda":
                torch.cuda.synchronize()
            t0 = time.perf_counter()
            vectors = embedding_fn.encode_documents(docs_as_strings)
            if device == "cuda":
                torch.cuda.synchronize()
            embed_elapsed_total += (time.perf_counter() - t0)

            if len(vectors) != len(batch_items):
                return {"success": False, "message": "âŒ ë²¡í„°/ë°ì´í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜(ë°°ì¹˜)"}

            buffer_rows = []
            buffer_bytes = 0
            include_text = ("text" in field_names)

            for i, item in enumerate(batch_items):
                row = build_row_for_schema(item, vectors[i], field_names, varchar_limits)
                row_bytes = approx_row_bytes(vectors[i], item, include_text=include_text)

                if buffer_rows and (buffer_bytes + row_bytes) > req.max_payload_bytes:
                    t1 = time.perf_counter()
                    client.insert(collection_name=req.collection_name, data=buffer_rows)
                    insert_elapsed_total += (time.perf_counter() - t1)
                    inserted_count += len(buffer_rows)
                    buffer_rows = []
                    buffer_bytes = 0

                buffer_rows.append(row)
                buffer_bytes += row_bytes

            if buffer_rows:
                t1 = time.perf_counter()
                client.insert(collection_name=req.collection_name, data=buffer_rows)
                insert_elapsed_total += (time.perf_counter() - t1)
                inserted_count += len(buffer_rows)

        stats = client.get_collection_stats(req.collection_name)
        total_count = int(stats["row_count"])

        total_elapsed = time.perf_counter() - total_start

        return {
            "success": True,
            "message": (
                f"ğŸ‰ {inserted_count}ê°œ ì—”í‹°í‹° ì‚½ì… ì™„ë£Œ "
                f"(â±ï¸ ì„ë² ë”© {embed_elapsed_total:.2f}s, ì‚½ì… {insert_elapsed_total:.2f}s, ì „ì²´ {total_elapsed:.2f}s; "
                f"ë°°ì¹˜ embed={req.embed_batch_size}, payloadâ‰¤{req.max_payload_bytes // (1024*1024)}MB)"
            ),
            "total_entities": total_count
        }

    except Exception as e:
        return {"success": False, "message": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"}

##########################################################################################################################################################
##########################################################################################################################################################
##########################################################################################################################################################

#########################################################################################
################# ê²€ìƒ‰ #################

# ğŸ§  ê³µí†µ ì„ë² ë”© í•¨ìˆ˜
embedding_fn = SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/all-mpnet-base-v2",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# âœ… 1. ê¸°ë³¸ ê²€ìƒ‰ API
@router.get("/search_basic")
async def search_basic_api(
    query_text: str = Query(..., description="ê²€ìƒ‰í•  í…ìŠ¤íŠ¸"),
    collection_name: str = Query(..., description="Milvus ì»¬ë ‰ì…˜ ì´ë¦„"),
    top_k: int = Query(3, description="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ Kê°œ")
):
    try:
        print("ì‹œì‘í•´ë³´ì.")
        start_time = time.time()

        # ğŸ”¹ ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = embedding_fn.encode_queries([query_text])[0]

        # ğŸ”¹ ì»¬ë ‰ì…˜ í•„ë“œ í™•ì¸
        schema_info = client.describe_collection(collection_name)
        field_names = {f["name"] for f in schema_info["fields"]}

        output_fields = ["id", "text"] if "text" in field_names else ["id", "code", "file_path"]

        # ğŸ”¹ ë²¡í„° ê²€ìƒ‰
        results = client.search(
            collection_name=collection_name,
            data=[query_vector],
            anns_field="embedding",
            search_params={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=top_k,
            output_fields=output_fields
        )

        elapsed = time.time() - start_time
        response_results = []

        for hit in results[0]:
            entity = hit.get("entity", {})
            response_results.append({
                "id": entity.get("id"),
                "text": entity.get("text") or entity.get("code", ""),
                "distance": hit["distance"]
            })

        return {
            "success": True,
            "message": f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ (Top-{top_k}, â±ï¸ {elapsed:.2f}ì´ˆ)",
            "results": response_results
        }

    except Exception as e:
        return {"success": False, "message": f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}


# âœ… 2. ë©”íƒ€ë°ì´í„° í•„í„° ê²€ìƒ‰ API
@router.get("/search_with_metadata")
async def search_with_metadata_filter_api(
    query_text: str = Query(..., description="ê²€ìƒ‰í•  í…ìŠ¤íŠ¸"),
    collection_name: str = Query(..., description="Milvus ì»¬ë ‰ì…˜ ì´ë¦„"),
    metadata_filter: str = Query("type like \"%module%\"", description="í•„í„° ì¡°ê±´ (ì˜ˆ: type like \"%module%\")"),
    top_k: int = Query(3, description="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ Kê°œ")
):
    try:
        start_time = time.time()

        query_vector = embedding_fn.encode_queries([query_text])[0]

        # ğŸ”¹ í•„ë“œ ì„¤ì •
        output_fields = ["id", "code", "file_path", "type", "name", "start_line", "end_line"]

        # ğŸ”¹ í•„í„° ì ìš© ë²¡í„° ê²€ìƒ‰
        results = client.search(
            collection_name=collection_name,
            data=[query_vector],
            anns_field="embedding",
            search_params={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=top_k,
            filter=metadata_filter,
            output_fields=output_fields
        )

        elapsed = time.time() - start_time
        response_logs = []

        for idx, hit in enumerate(results[0], 1):
            entity = hit.get("entity", {})

            # ğŸ“Œ ì¤„ë°”ê¿ˆÂ·í°íŠ¸ ê³ ì •ìš© ë¬¸ìì—´ ìƒì„±
            formatted = "\n".join([
                f"[{idx}] ID: {entity.get('id', '')} | ìœ ì‚¬ë„: {hit['distance']:.4f}",
                f"ğŸ“‚ Path: {entity.get('file_path', '')}",
                f"ğŸ”– Type: {entity.get('type', '')} | Name: {entity.get('name', '')}",
                f"ğŸ“ Lines: {entity.get('start_line', '?')} - {entity.get('end_line', '?')}",
                "ğŸ’» Code Preview:",
                f"```python\n{(entity.get('code') or '')[:500]}\n```",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            ])

            response_logs.append({
                "rank": idx,
                "id": entity.get("id"),
                "distance": hit["distance"],
                "file_path": entity.get("file_path", ""),
                "type": entity.get("type", ""),
                "name": entity.get("name", ""),
                "start_line": entity.get("start_line", "?"),
                "end_line": entity.get("end_line", "?"),
                "code_preview": (entity.get("code") or "")[:300],
                "log_str": formatted  # ğŸ”¹ ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ë¬¸ìì—´
            })

        return {
            "success": True,
            "message": f"ğŸ” ë©”íƒ€ë°ì´í„° í•„í„° ê²€ìƒ‰ ì™„ë£Œ (Top-{top_k}, â±ï¸ {elapsed:.2f}ì´ˆ)",
            "results": response_logs
        }

    except Exception as e:
        return {"success": False, "message": f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}

    


############################### total json ì„ì‹œ ìƒì„±

# â”€â”€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ìë™ íƒìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_project_root(marker_dir_name="git-ai") -> Path:
    """marker_dir_name ë””ë ‰í† ë¦¬ê¹Œì§€ ìƒìœ„ë¡œ íƒìƒ‰í•˜ì—¬ Path ë°˜í™˜"""
    path = Path(__file__).resolve()
    for parent in path.parents:
        if parent.name == marker_dir_name:
            return parent
    raise RuntimeError(f"âŒ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ '{marker_dir_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

PROJECT_ROOT = find_project_root("git-ai")
TARGET_ROOT = PROJECT_ROOT / "git-agent" / "parsed_repository"

def iter_json_items(p: Path):
    """íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ë©´ ì›ì†Œë¥¼, ì˜¤ë¸Œì íŠ¸ë©´ ê·¸ ìì²´ë¥¼ yield. ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ dict."""
    try:
        with p.open("r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            for item in data:
                yield item
        else:
            yield data
    except Exception as e:
        yield {"_error": f"failed_to_parse:{p.name}", "_reason": str(e)}

@router.post("/merge-json")
def merge_json(repo: str):
    target_dir = (TARGET_ROOT / repo).resolve()

    # TARGET_ROOT ë°”ê¹¥ì„ ê°€ë¦¬í‚¤ëŠ” ìš°íšŒ ë°©ì§€
    if TARGET_ROOT not in target_dir.parents and target_dir != TARGET_ROOT:
        raise HTTPException(status_code=400, detail="Invalid repo path.")

    if not target_dir.exists() or not target_dir.is_dir():
        raise HTTPException(status_code=404, detail=f"Not found: {target_dir}")

    json_files = sorted(target_dir.rglob("*.json"))
    if not json_files:
        raise HTTPException(status_code=404, detail=f"No JSON files under: {target_dir}")

    out_path = (target_dir / f"{repo}__all.json").resolve()
    out_path.parent.mkdir(parents=True, exist_ok=True)

    total = 0
    skipped = 0
    merged: List[Dict[str, Any]] = []

    for jp in json_files:
        for item in iter_json_items(jp):
            if isinstance(item, dict) and "_error" in item:
                skipped += 1
                continue
            if isinstance(item, dict):
                item.setdefault("_source_file", str(jp.relative_to(target_dir)))
            merged.append(item)
            total += 1

    with out_path.open("w", encoding="utf-8") as out:
        json.dump(merged, out, ensure_ascii=False, indent=2)

    return {
        "repo": repo,
        "out_path": str(out_path),
        "files_scanned": len(json_files),
        "merged_items": total,
        "skipped": skipped
    }
